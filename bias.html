<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Big Idea 5.3 Computing Bias | aniket</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Big Idea 5.3 Computing Bias" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="This tech talk discusses bias in computing" />
<meta property="og:description" content="This tech talk discusses bias in computing" />
<link rel="canonical" href="https://anicricket.github.io/automatic-spork/bias" />
<meta property="og:url" content="https://anicricket.github.io/automatic-spork/bias" />
<meta property="og:site_name" content="aniket" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2023-02-01T00:00:00-06:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Big Idea 5.3 Computing Bias" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2023-02-01T00:00:00-06:00","datePublished":"2023-02-01T00:00:00-06:00","description":"This tech talk discusses bias in computing","headline":"Big Idea 5.3 Computing Bias","mainEntityOfPage":{"@type":"WebPage","@id":"https://anicricket.github.io/automatic-spork/bias"},"url":"https://anicricket.github.io/automatic-spork/bias"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/automatic-spork/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://anicricket.github.io/automatic-spork/feed.xml" title="aniket" /><link rel="shortcut icon" type="image/x-icon" href="/automatic-spork/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />

<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/automatic-spork/">aniket</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/automatic-spork/about/">About</a><a class="page-link" href="/automatic-spork/Collegeboard/">Collegeboard</a><a class="page-link" href="/automatic-spork/Notes/">Notes</a><a class="page-link" href="/automatic-spork/search/">Search</a><a class="page-link" href="/automatic-spork/Submenu">Submenu</a><a class="page-link" href="/automatic-spork/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Big Idea 5.3 Computing Bias</h1><p class="page-description">This tech talk discusses bias in computing</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2023-02-01T00:00:00-06:00" itemprop="datePublished">
        Feb 1, 2023
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      3 min read
    
</span></p>

    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul id="toc" class="section-nav">
<li class="toc-entry toc-h1"><a href="#computer-bias">Computer Bias</a>
<ul>
<li class="toc-entry toc-h3"><a href="#intentional-or-purposeful-bias-crossover-group-up-10-minutes">Intentional or Purposeful bias (Crossover Group Up, 10 minutes)</a></li>
<li class="toc-entry toc-h3"><a href="#as-pairs-5-minutes">As Pairs (5 minutes)</a></li>
<li class="toc-entry toc-h2"><a href="#hacks">Hacks</a></li>
</ul>
</li>
</ul><h1 id="computer-bias">
<a class="anchor" href="#computer-bias" aria-hidden="true"><span class="octicon octicon-link"></span></a>Computer Bias</h1>
<p>Earlier we talked about beneficial and harmful effects of computing.  Such conversation often lead to conversations on computer bias, particularly when bias creates a harmful effect.</p>

<p>As programmers, you now have the possibility of creating algorithms.  It has been said, “Humans are error-prone and biased”.  So, does that mean algorithms and the computers they run on are better?</p>

<h3 id="intentional-or-purposeful-bias-crossover-group-up-10-minutes">
<a class="anchor" href="#intentional-or-purposeful-bias-crossover-group-up-10-minutes" aria-hidden="true"><span class="octicon octicon-link"></span></a>Intentional or Purposeful bias (Crossover Group Up, 10 minutes)</h3>
<ul>
  <li>Google “What age groups use Facebook” vs “… TikTok”?  What does the data say?  Is there purposeful exclusion in these platforms?  Is it harmful?  Should it be corrected?  Is it good business?</li>
</ul>

<p>Older people use Facebook while the younger generation uses TikTok. Data says that more than 41% of the users on Facebook are 18-34 years. Meanwhile around 60% of users on tik tok are between the ages of 16-24. It can be slightly harmful as companies use this information and show more targeted ads.</p>

<ul>
  <li>Why do virtual assistants have female voices? Amazon, Alexa Google, Apple Siri. Was this purposeful?  Is it harmful?  Should it be corrected?  Is it good business?</li>
</ul>

<p>The reason virtual assistants have female voices is because that’s what people thought was more appealing and friendly back when these technologies were first developed. Some people think that it’s a bad thing because it reinforces gender stereotypes, while others think it’s fine. Companies are starting to offer different voice options, including male and gender-neutral voices, to be more inclusive.</p>

<ul>
  <li>Talk about an algorithm that influences your decisions, think about these companies (ie FAANG - Facebook, Amazon, Apple,Netflix, Google)</li>
</ul>

<p>An example of an algorithm that affects decisions is the recommendation algorithm used by Netflix and Amazon to suggest personalized content or products. It analyzes user behavior to show items they’re more likely to enjoy or buy.</p>

<h3 id="as-pairs-5-minutes">
<a class="anchor" href="#as-pairs-5-minutes" aria-hidden="true"><span class="octicon octicon-link"></span></a>As Pairs (5 minutes)</h3>
<ul>
  <li>Watch the video… <a href="https://www.youtube.com/watch?v=t4DT3tQqgRM">HP computers are racist</a>
</li>
  <li>The speaker claims that an HP media smart computer’s face tracking software is racist as it does not follow them when they move in front of the camera but does follow their white co-worker. The speaker says they bought the same computer for Christmas and hopes their wife doesn’t see the video, as the computer does not follow them and only follows their white co-worker.
    <ul>
      <li>Does the owner of the computer think this was intentional?
Based on the video, it’s not explicitly stated if the owner of the computer thinks this was intentional or not. However, they do make a statement that they believe HP computers are racist.</li>
      <li>How do you think this happened?
This probably happened because of a lack of diversity in the testing and development process, leading to potential biases in the software. It could also be due to a lack of proper calibration or technical difficulties with the hardware.</li>
      <li>Is this harmful?  Was it intended to be harmful or exclude?
Yes, this can be harmful as it creates an exclusionary experience for the individual who is black and is not being properly recognized by the computer. This can lead to feelings of frustration, anger, and discrimination. I personally don’t think it was intensional.</li>
      <li>Should it be corrected?
Yes, it should be corrected. Technology should be inclusive and accessible to all, regardless of race or ethnicity. This can be done by increasing diversity in the development and testing process, ensuring that the software is properly calibrated and tested, and by implementing measures to address and correct any biases in the technology.</li>
      <li>What would you or should you do to produce a better outcome?
I would test with all different types of people and make sure that it works with a diverse group of people.</li>
    </ul>
  </li>
</ul>

<h2 id="hacks">
<a class="anchor" href="#hacks" aria-hidden="true"><span class="octicon octicon-link"></span></a>Hacks</h2>
<blockquote>
  <p>Write summary/thoughts/conclusions from each of the exercises above.  Focus on avoiding Bias in algorithms or code you write.</p>
</blockquote>

  </div><a class="u-url" href="/automatic-spork/bias" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/automatic-spork/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="https://anicricket.github.io/automatic-spork/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/automatic-spork/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>My CS portfolio, made with Jupyter Notebooks.</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"></ul>
</div>

  </div>

</footer>
</body>

</html>
